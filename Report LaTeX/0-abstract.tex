\chapter*{Abstract}
In geosciences, a predominant challenge is to reconstruct the thermochemical evolution of Earth's mantle along with its various surface manifestations, where the insights into previous mantle states are essential. To this end, 
the framework of inverse modelling is introduced, whose main solution strategy is the adjoint method. However, this approach is computationally intensive. To significantly reduce computational requirements, we explore deep learning methods as surrogate models to approximate the forward and adjoint problems. Two well-established geodynamics problems are addressed as our attempt in this direction, that is, the geoid problem and the 2D Mantle convection problems. For the geoid problem, we use a reduced dataset of 1000 pairs of viscosity model and geoid coefficients and show that a feed-forward Fully Connected Neural Network (FNN) is able to help with the forward modelling of a reduced geoid problem with limited amounts of data. As for the mantle convection problem, we have tested three datasets where each dataset is composed of several temperature time-series with 100 time steps each resulting from numerical simulations with randomly sampled initial conditions (except for the last dataset, which is created using interpolation on the second dataset). For each of these datasets, we first use Convolutional AutoEncoders (ConvAE) to compress the size of each temperature field by a factor of 13 and then FNN and Long-Short Term Memory (LSTM) networks to predict the compressed fields at the next time steps given an initial temperature field. We found that while FNN is better for predicting temperature fields which are closely located in time, LSTM is slightly more capable to capture the transient dynamics of a complete simulation, thus being better for predicting fields further away in time. We also found that when FNN is used for a number of consecutive steps, $S$, before it is corrected using a temperature field produced by the numerical solver (i.e., our ``ground-truth'' in this research), the growth of the data loss with respect to $S$ is remarkably moderate (less than $O(S)$), with only a few outliers deviating moderately from the median.