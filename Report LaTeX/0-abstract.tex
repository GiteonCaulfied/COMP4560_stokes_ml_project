\chapter*{Abstract}
In geosciences, a predominant challenge is to reconstruct the thermochemical evolution of Earth's mantle along with its various surface manifestations, where the insights into previous mantle states are essential. The concept of the inverse problem is thus introduced, whose main strategy is called adjoint method. However, this approach is computationally intensive. To significantly reduce the computational requirement, we aim to use deep learning methods as surrogate models to approximate the forward and adjoint problems. Two well-established geodynamics problems are addressed as our attempt in this direction, that is, the geoid problem and the 2D Mantle convection problems. For the geoid problem, we use a reduced dataset of 1000 pairs of viscosity model and geoid coefficients and shows that a feed-forward Fully Connected Neural Network (FNN) is able to help with the forward modelling of a reduced geoid problem without numerous amount of data. As for the mantle convection problem, we have tested three datasets where each dataset is composed of several temperature time-series with 100 time steps each resulting from numerical simulations with randomly sampled initial conditions (except for the last dataset, which is created using interpolation on the second dataset). For each of these datasets, we first use Convolutional AutoEncoders (ConvAE) to compress the size of each temperature field by a factor of 13 and and then use FNN and Long-Short Term Memory (LSTM) to predict the compressed fields at the next time steps given some initial temperature fields. We found that while FNN offers a better accuracy and the result has less data loss when it uses a single temperature field to predict the one in its subsequent time step, LSTM is slightly more capable to capture the general characteristics of a complete simulation when predicting a sequence of temperature fields, compared with the result when FNN uses one initial temperature to predict the rest 99 of them in a simulation. We also found that when FNN is used for a number of consecutive steps, S, before it is corrected using a ground truth, the growth of the data loss with respect to S is remarkably moderate (less than O(S)), with only a few outliers deviating moderately from the median.