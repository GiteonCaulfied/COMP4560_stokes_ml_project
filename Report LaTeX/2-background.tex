\chapter{Background}\label{chap:background}


\section{Neural networks: Fully connected, convolutional and long short-term memory}

In this section, we introduce some basics about neural networks (NNs) according to a more detailed explanation provided by Bishop. \citep{10.1117/1.2819119} A simple NN usually consists of one input layer, 
one output layer and one or more hidden layers in between. Multiple nodes inside each layers are called neurons. 

For a fully connected neural network (FNN) using linear layers for connection (the term “fully connected” means that the neurons in each layer have a connection to all neurons in the previous layer and also the following layer), each neuron receives all inputs from the previous layer and the outputs are as follows:

\begin{equation}
z = g(xA^{T} + b)
\end{equation}

In this case, $z$ represents a vector contains the result values of all the nodes in the current layer and $x$ represents a vector contains the values of all the nodes in the previous layer. $g()$ represents an activation function allows the NN to model non-linearity. $A$ is the weight matrix and $b$ is the bias vector from the linear layer (It is called a linear layer since it applies a linear transformation to the incoming data). Both $A$ and $b$ are able to be optimized through a technique called error backpropagation, where we first define a loss function calculate the loss value (error) between the predicted output from the output layer and the actual output from the data set. The error is then propagated backwards through all the hidden layers using chain rule to perform differentiation. Eventually, these derivatives of errors with respect to the weights are used to update the learnable parameters ($A$ and $b$) in each hidden layer. This complete process is called gradient descent. 

By looping over the process of feeding the input data into NN to perform prediction, calculate the loss between the prediction and the actual data, use the loss value to optimize the learnable parameters in each layers, the NN model is able to be adjusted to a state that best fits the underlying pattern of the provided data set given its current structure.

In this study, we use a ML library called PyTorch to define the NN architecture, specifying the loss function and optimizer that minimize the loss function (Adam optimizer, in this case, is used throughout the study) and systematically train and test the performance of the networks.

Apart from FNNs, Convolutional neural networks (CNNs) are also one of the most commonly used NNs. They can handle matrix or image input better than the traditional FNN with linear layers. Instead of linear layers, they use convolutional layers that contains a specified number of trainable filters, each of which is used to enhance or identify a particular feature in the input. Filters are convolved with the input image or the output of the previous layer, and the results are summed together along with a trainable bias and passed through an activation function to produce a feature map. Because convolution is also a linear operation, activation function is added to introduce non-linearity to the feature map like FNN.

In this study, a variation of CNN is used as a way to compress the size of the input data in the mantle convection problem, which is called Convolutional Autoencoder (ConvAE). It is constructed as two separate sturcture that trained together: an encoder using convolution operation to reduce the size of the original input field and output a latent space representation, and a decoder using deconvolution operation to transform the latent space representation back to the original size field. By feeding the input field into a ConvAE, the dimension of the original high-resolution fields can be decreased with the main features captured (some information may be lost during the encoding process), thus making it more computationally efficient to work with before we feed it into a prediction NN.

Since the data set in the mantle convection problem is a time sequence with adaptive timestamps, long short-term memory (LSTM) is used to predict a sequence of output apart from FNN. LSTM's architecture that use a sequence of input recurrently during prediction allows it to handle time-series data more accurately than other networks, thus leading to a potential better result.


\section{Related works for solving geoid and mantle convection using Neural networks}

(TODO...)