{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb96a2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import os\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb9ae21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100, 201, 401)\n"
     ]
    }
   ],
   "source": [
    "# Temperature for the two consecutive timestamp\n",
    "temperature_fields = []\n",
    "\n",
    "# Folder Path\n",
    "path = \"solutions\"\n",
    "  \n",
    "# Read text File  \n",
    "def read_text_file(file_path):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        temperature_fields.append(f['temperature'][:])\n",
    "        \n",
    "        \n",
    "# Iterate through all file\n",
    "for file in os.listdir(path):\n",
    "    file_path = f\"{path}/{file}\"\n",
    "  \n",
    "    # call read text file function\n",
    "    read_text_file(file_path)\n",
    "    #print(f\"{file_path} is finished reading\")\n",
    "\n",
    "temperature_fields = np.asarray(temperature_fields)\n",
    "print(temperature_fields.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54a22253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "n_epoch = 200\n",
    "batch_size = 1\n",
    "lr = 5e-5\n",
    "betas = (0.9, 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1775e7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device is  cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "print(\"Current device is \",device)\n",
    "\n",
    "# make results determinstic\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be97f22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential( # 1x201x401 => 6x23x45\n",
    "            nn.Conv2d(1, 3, stride=(3, 3), kernel_size=(5, 5), padding=2),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(3, 6, stride=(3, 3), kernel_size=(5, 5), padding=2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out=self.encoder(x)\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.decoder = nn.Sequential( # 6x23x45 => 201x401\n",
    "            nn.ConvTranspose2d(6, 3, stride=(3, 3), kernel_size=(5, 5), padding=(2,2)),\n",
    "            nn.Tanh(),\n",
    "            nn.ConvTranspose2d(3, 1, stride=(3, 3), kernel_size=(5, 5), padding=(1,0)),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out=self.decoder(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e03b268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder and Decoder loaded!\n"
     ]
    }
   ],
   "source": [
    "encoder_path = \"2D_ConvAE_results/Conv2D_encoder_best_Gadi.pth\"\n",
    "decoder_path = \"2D_ConvAE_results/Conv2D_decoder_best_Gadi.pth\"\n",
    "\n",
    "encoder = Encoder().to(device)\n",
    "decoder = Decoder().to(device)\n",
    "encoder.load_state_dict(torch.load(encoder_path, map_location=torch.device('cpu')))\n",
    "decoder.load_state_dict(torch.load(decoder_path, map_location=torch.device('cpu')))\n",
    "\n",
    "print(\"Encoder and Decoder loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecddac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customised Dataset class\n",
    "class KMNIST(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.input = dataset[:,:50,:,:]\n",
    "        self.output = dataset[:,50:,:,:]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_item = self.input[index]\n",
    "        output_item = self.output[index]\n",
    "        \n",
    "        return input_item, output_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b0cea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_dataset = KMNIST(\n",
    "    temperature_fields\n",
    ")\n",
    "\n",
    "\n",
    "testingAndValidation_split = 0.2\n",
    "validation_split = 0.1\n",
    "\n",
    "# Creating data indices for training, testing and validation splits\n",
    "# Reference: https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets\n",
    "temperature_dataset_size = len(temperature_dataset)\n",
    "temperature_indices = list(range(temperature_dataset_size))\n",
    "\n",
    "temperature_training_testing_split = int(np.floor(testingAndValidation_split * temperature_dataset_size))\n",
    "temperature_testing_validation_split = int(np.floor(validation_split * temperature_dataset_size))\n",
    "\n",
    "np.random.shuffle(temperature_indices)\n",
    "temperature_train_indices, temperature_val_indices ,temperature_test_indices = temperature_indices[temperature_training_testing_split:], temperature_indices[:temperature_testing_validation_split], temperature_indices[temperature_testing_validation_split:temperature_training_testing_split] \n",
    "\n",
    "# Creating data samplers\n",
    "temperature_train_sampler = SubsetRandomSampler(temperature_train_indices)\n",
    "temperature_test_sampler = SubsetRandomSampler(temperature_test_indices)\n",
    "temperature_valid_sampler = SubsetRandomSampler(temperature_val_indices)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=temperature_dataset,\n",
    "    batch_size = batch_size,\n",
    "    sampler=temperature_train_sampler,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=temperature_dataset,\n",
    "    batch_size = batch_size,\n",
    "    sampler=temperature_test_sampler,\n",
    ")\n",
    "\n",
    "validation_loader = DataLoader(\n",
    "    dataset=temperature_dataset,\n",
    "    batch_size = batch_size,\n",
    "    sampler=temperature_valid_sampler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4432955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 6210])\n",
      "torch.Size([50, 6, 23, 45])\n",
      "torch.Size([1, 50, 6210])\n",
      "torch.Size([50, 6, 23, 45])\n",
      "torch.Size([1, 50, 6210])\n",
      "torch.Size([50, 6, 23, 45])\n",
      "torch.Size([1, 50, 6210])\n",
      "torch.Size([50, 6, 23, 45])\n",
      "torch.Size([1, 50, 6210])\n",
      "torch.Size([50, 6, 23, 45])\n",
      "torch.Size([1, 50, 6210])\n",
      "torch.Size([50, 6, 23, 45])\n",
      "torch.Size([1, 50, 6210])\n",
      "torch.Size([50, 6, 23, 45])\n",
      "torch.Size([1, 50, 6210])\n",
      "torch.Size([50, 6, 23, 45])\n",
      "torch.Size([1, 50, 6210])\n",
      "torch.Size([50, 6, 23, 45])\n",
      "torch.Size([1, 50, 6210])\n",
      "torch.Size([50, 6, 23, 45])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for X, Y in test_loader:\n",
    "    print(encoder(X.view(X.shape[1], 1, 201, 401)).reshape(batch_size,50,-1).shape)\n",
    "    print(encoder(Y.view(Y.shape[1], 1, 201, 401)).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b34c805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()    \n",
    "        self.lstm1 = nn.LSTM(input_size=6210, hidden_size=3105, num_layers=1, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=3105, hidden_size=6210, num_layers=1, batch_first=True)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        sequence_length = 50\n",
    "        \n",
    "        out, _ = self.lstm1(x)\n",
    "        out, _ = self.lstm2(out)\n",
    "        \n",
    "        out = out.view(sequence_length,6,23,45)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b754db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, encoder, train_loader, val_loader, device, optimizer, n_epoch):\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    minimum_validation_loss = 10000000\n",
    "    best_model_index = -1\n",
    "    \n",
    "    running_loss_list = []\n",
    "    validation_loss_list = []\n",
    "\n",
    "    # n_epoch times of iterations\n",
    "    for epoch in range(n_epoch):\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        for data in train_loader:\n",
    "            # get a batch of inputs and labels\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            encoded_inputs = encoder(inputs.view(inputs.shape[1], 1, 201, 401)).reshape(batch_size, 50, -1)\n",
    "            encoded_labels = encoder(labels.view(labels.shape[1], 1, 201, 401))\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Get output features, calculate loss and optimize\n",
    "            outputs = model(encoded_inputs)\n",
    "            loss = criterion(outputs.float(), encoded_labels.float())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Add to the total training loss\n",
    "            running_loss += loss.item()\n",
    "            print(loss.item())\n",
    "\n",
    "        # print some statistics\n",
    "        print(epoch+1,\"epochs have finished\")\n",
    "        print(\"Current training loss is \",running_loss)\n",
    "        running_loss_list.append(running_loss)\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Valiadation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = 0.0\n",
    "            for data in val_loader:\n",
    "                # get a batch of inputs and labels\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                encoded_inputs = encoder(inputs.view(inputs.shape[1], 1, 201, 401)).reshape(batch_size, 50, -1)\n",
    "                encoded_labels = encoder(labels.view(labels.shape[1], 1, 201, 401))\n",
    "\n",
    "                # Get output features, calculate loss and optimize\n",
    "                outputs = model(encoded_inputs)\n",
    "                loss = criterion(outputs.float(), encoded_labels.float())\n",
    "\n",
    "                # Add to the validation loss\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "            # Calculate valiadation accuracy and print Validation statistics\n",
    "            print(\"Validation loss for this epoch is\",valid_loss)\n",
    "            validation_loss_list.append(valid_loss)\n",
    "\n",
    "        # Update the statistics for the best model\n",
    "        if valid_loss <= minimum_validation_loss:\n",
    "            minimum_validation_loss = valid_loss\n",
    "\n",
    "            # Store the best models\n",
    "            PATH = 'lstm_best.pth'\n",
    "\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            print(\"This model is now saved to Path:\",PATH)\n",
    "            \n",
    "            best_model_index = epoch\n",
    "            \n",
    "        print()\n",
    "    \n",
    "    # Training finished, print the statistics for the best model\n",
    "    print('Finished Training')\n",
    "    print(\"Best model has a validation loss of \",minimum_validation_loss)\n",
    "    print(\"Best model is in epoch \",best_model_index+1)\n",
    "    \n",
    "    # Plot the Training loss and validation loss during training\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(running_loss_list)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.title('Training Loss in each epoch')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(validation_loss_list)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.title('Validation Loss in each epoch')\n",
    "    \n",
    "    plt.subplots_adjust(hspace=1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d16c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06d39d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2614653408527374\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-84fd0f1a3145>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-a38a41e056b2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, encoder, train_loader, val_loader, device, optimizer, n_epoch)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# Get output features, calculate loss and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-b434874bffce>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    813\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    814\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, encoder, train_loader, validation_loader, device, optimizer, n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4285a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, encoder, decoder, test_loader, device):\n",
    "\n",
    "    # Load the model from the input model_path  \n",
    "    model.load_state_dict(torch.load('lstm_best.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    best_worst_error_list = [1000000, 0]\n",
    "    best_worst_output_list = [0, 0]\n",
    "    best_worst_predicted_list = [0, 0]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            encoded_inputs = encoder(inputs.view(inputs.shape[1], 1, 201, 401)).reshape(batch_size, 50, -1)\n",
    "            encoded_labels = encoder(labels.view(labels.shape[1], 1, 201, 401))\n",
    "\n",
    "            # Get output features, calculate loss and optimize\n",
    "            outputs = model(encoded_inputs)\n",
    "            loss = criterion(outputs.float(), encoded_labels.float())\n",
    "\n",
    "            for j in range(len(encoded_labels)):\n",
    "                single_loss = criterion(outputs[j], encoded_labels[j])\n",
    "                # Record worst error\n",
    "                if single_loss.item() > best_worst_error_list[1]:\n",
    "                    best_worst_error_list[1] = single_loss.item()\n",
    "                    best_worst_output_list[1] = labels[0][j]\n",
    "                    best_worst_predicted_list[1] = outputs[j]\n",
    "                    \n",
    "                # Record best error\n",
    "                if single_loss.item() < best_worst_error_list[0]:\n",
    "                    best_worst_error_list[0] = single_loss.item()\n",
    "                    best_worst_output_list[0] = labels[0][j]\n",
    "                    best_worst_predicted_list[0] = outputs[j]\n",
    "                    \n",
    "\n",
    "            # Add to the validation loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    # Calculate the overall accuracy and return the accuracy and test loss\n",
    "    print(\"Total loss for the model is\",total_loss)\n",
    "    print()\n",
    "    \n",
    "    # Draw some plots for the best and the worst error\n",
    "    print(\"Best model has a error of \", best_worst_error_list[0])\n",
    "    \n",
    "    plt.figure(figsize=(18, 9))\n",
    "    \n",
    "    plt.subplot(2,2,1)\n",
    "    plt.title(\"Best case output\")\n",
    "    plt.imshow(best_worst_output_list[0].detach().numpy(),\n",
    "              cmap=cm.get_cmap('jet', 10),\n",
    "              extent=(0, 2, 0, 1))\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim(ax.get_ylim()[::-1])\n",
    "    \n",
    "    \n",
    "    original_size_predicted = decoder(encoder(best_worst_output_list[0].view(1, 1, 201, 401)).view(1, 6, 23, 45))\n",
    "    \n",
    "    plt.subplot(2,2,2)\n",
    "    plt.title(\"Best case AE output\")\n",
    "    plt.imshow(original_size_predicted.detach().numpy()[0][0],\n",
    "              cmap=cm.get_cmap('jet', 10),\n",
    "              extent=(0, 2, 0, 1))\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim(ax.get_ylim()[::-1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    latent_space_predicted = best_worst_predicted_list[0].view(1, 6, 23, 45)\n",
    "    original_size_predicted = decoder(latent_space_predicted)\n",
    "    \n",
    "    plt.subplot(2,2,3)\n",
    "    plt.title(\"Best case predicted output\")\n",
    "    plt.imshow(original_size_predicted.detach().numpy()[0][0],\n",
    "              cmap=cm.get_cmap('jet', 10),\n",
    "              extent=(0, 2, 0, 1))\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim(ax.get_ylim()[::-1])\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print(\"Worst model has a error of \", best_worst_error_list[1])\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(18, 9))\n",
    "    \n",
    "    plt.subplot(2,2,1)\n",
    "    plt.title(\"Worst case output\")\n",
    "    plt.imshow(best_worst_output_list[1].detach().numpy(),\n",
    "              cmap=cm.get_cmap('jet', 10),\n",
    "              extent=(0, 2, 0, 1))\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim(ax.get_ylim()[::-1])\n",
    "    \n",
    "    \n",
    "    original_size_predicted = decoder(encoder(best_worst_output_list[1].view(1, 1, 201, 401)).view(1, 6, 23, 45))\n",
    "    \n",
    "    plt.subplot(2,2,2)\n",
    "    plt.title(\"Worst case AE output\")\n",
    "    plt.imshow(original_size_predicted.detach().numpy()[0][0],\n",
    "              cmap=cm.get_cmap('jet', 10),\n",
    "              extent=(0, 2, 0, 1))\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim(ax.get_ylim()[::-1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    latent_space_predicted = best_worst_predicted_list[1].view(1, 6, 23, 45)\n",
    "    original_size_predicted = decoder(latent_space_predicted)\n",
    "    \n",
    "    plt.subplot(2,2,3)\n",
    "    plt.title(\"Worst case predicted output\")\n",
    "    plt.imshow(original_size_predicted.detach().numpy()[0][0],\n",
    "              cmap=cm.get_cmap('jet', 10),\n",
    "              extent=(0, 2, 0, 1))\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim(ax.get_ylim()[::-1])\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    #return 100*correct//total, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38d1c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, encoder, decoder, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75a8038",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
